# è©•ä¾¡å™¨ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ - Factory Patternå®Ÿè£…

from typing import Dict, List, Any, Optional
from .base import BaseEvaluator
from .ragas_ollama import RagasOllamaEvaluator  # Re-enabled with compatible versions

class EvaluatorFactory:
    """è©•ä¾¡å™¨ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‚¯ãƒ©ã‚¹ - è©•ä¾¡å™¨ã®ç”Ÿæˆã¨ç®¡ç†ã‚’çµ±ä¸€"""
    
    # åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡å™¨ã‚¿ã‚¤ãƒ—
    EVALUATOR_TYPES = {
        "ragas": RagasOllamaEvaluator,  # Re-enabled with compatible versions
        # "academic": AcademicEvaluator  # Removed in favor of async version
    }
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè©•ä¾¡å™¨ã®å„ªå…ˆé †ä½
    DEFAULT_PRIORITY = ["ragas"]  # Re-enabled Ragas evaluator
    
    @classmethod
    def create_evaluator(cls, evaluator_type: str, config: Dict[str, Any]) -> Optional[BaseEvaluator]:
        """æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ—ã®è©•ä¾¡å™¨ã‚’ä½œæˆ"""
        if evaluator_type not in cls.EVALUATOR_TYPES:
            raise ValueError(f"æœªå¯¾å¿œã®è©•ä¾¡å™¨ã‚¿ã‚¤ãƒ—: {evaluator_type}")
        
        evaluator_class = cls.EVALUATOR_TYPES[evaluator_type]
        
        try:
            evaluator = evaluator_class(config)
            if evaluator.is_available():
                return evaluator
            else:
                print(f"âš ï¸  {evaluator_type}è©•ä¾¡å™¨ã¯åˆ©ç”¨ä¸å¯")
                return None
        except Exception as e:
            print(f"âŒ {evaluator_type}è©•ä¾¡å™¨ä½œæˆå¤±æ•—: {e}")
            return None
    
    @classmethod
    def create_all_evaluators(cls, config: Dict[str, Any], 
                            types: Optional[List[str]] = None) -> Dict[str, BaseEvaluator]:
        """å…¨ã¦ã®åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡å™¨ã‚’ä½œæˆ"""
        if types is None:
            types = cls.DEFAULT_PRIORITY
        
        evaluators = {}
        
        for evaluator_type in types:
            evaluator = cls.create_evaluator(evaluator_type, config)
            if evaluator:
                evaluators[evaluator_type] = evaluator
        
        return evaluators
    
    @classmethod
    def get_evaluator_info(cls) -> Dict[str, Dict[str, Any]]:
        """å…¨è©•ä¾¡å™¨ã®æƒ…å ±ã‚’å–å¾—"""
        info = {}
        
        for name, evaluator_class in cls.EVALUATOR_TYPES.items():
            # ä¸€æ™‚çš„ã«ãƒ€ãƒŸãƒ¼è¨­å®šã§æƒ…å ±ã‚’å–å¾—
            dummy_config = {
                "api_key": "dummy",
                "base_url": "dummy",
                "model": "dummy"
            }
            
            try:
                temp_evaluator = evaluator_class(dummy_config)
                info[name] = {
                    "name": temp_evaluator.name,
                    "supported_metrics": temp_evaluator.get_supported_metrics(),
                    "description": cls._get_evaluator_description(name)
                }
            except:
                info[name] = {
                    "name": name,
                    "supported_metrics": [],
                    "description": cls._get_evaluator_description(name)
                }
        
        return info
    
    @classmethod
    def _get_evaluator_description(cls, evaluator_type: str) -> str:
        """è©•ä¾¡å™¨ã®èª¬æ˜ã‚’å–å¾—"""
        descriptions = {
            "ragas": "Ragasæ¡†æ¶ - OpenRouter Chat + Ollama Embeddings",  # Re-enabled with compatible versions
            # "academic": "å­¦è¡“çš„è©•ä¾¡å™¨ - 4æ¬¡å…ƒå°‚é–€è©•ä¾¡ï¼ˆé–¢é€£æ€§ã€æ­£ç¢ºæ€§ã€å®Œæ•´æ€§ã€æ¸…æ™°åº¦ï¼‰"  # Removed in favor of async version
        }
        return descriptions.get(evaluator_type, "èª¬æ˜ãªã—")

class EvaluatorManager:
    """è©•ä¾¡å™¨ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ - Strategy Patternå®Ÿè£…"""
    
    def __init__(self, chat_config: Dict[str, Any], embedding_config: Dict[str, Any]):
        """è©•ä¾¡å™¨ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–"""
        # åˆå¹¶èŠå¤©å’ŒåµŒå…¥é…ç½®
        combined_config = {**chat_config, **embedding_config}
        self.config = combined_config
        self.evaluators = EvaluatorFactory.create_all_evaluators(combined_config)
        
        if not self.evaluators:
            raise ValueError("åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡å™¨ãŒã‚ã‚Šã¾ã›ã‚“")
        
        print(f"ğŸ”§ åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡å™¨: {list(self.evaluators.keys())}")
    
    def evaluate_all(self, questions: List[str], answers: List[str], 
                    ground_truths: List[str], contexts: List[List[str]] = None) -> Dict[str, Dict[str, List[float]]]:
        """å…¨è©•ä¾¡å™¨ã§è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        all_results = {}
        
        for evaluator_name, evaluator in self.evaluators.items():
            print(f"\nğŸ“Š {evaluator_name}è©•ä¾¡å™¨ã§è©•ä¾¡ä¸­...")
            
            try:
                metrics = evaluator.evaluate_answers(questions, answers, ground_truths, contexts)
                all_results[evaluator_name] = metrics
                print(f"    âœ… å®Œäº†")
            except Exception as e:
                print(f"    âŒ å¤±æ•—: {e}")
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åŸ‹ã‚ã‚‹
                default_metrics = {metric: [None] * len(answers) 
                                 for metric in evaluator.get_supported_metrics()}
                all_results[evaluator_name] = default_metrics
        
        return all_results
    
    def get_evaluator_summary(self) -> Dict[str, Any]:
        """è©•ä¾¡å™¨ã®æ¦‚è¦ã‚’å–å¾—"""
        summary = {
            "total_evaluators": len(self.evaluators),
            "available_evaluators": list(self.evaluators.keys()),
            "evaluator_details": {}
        }
        
        for name, evaluator in self.evaluators.items():
            summary["evaluator_details"][name] = evaluator.get_info()
        
        return summary
