#!/usr/bin/env python3
# RAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - å¤šè©•ä¾¡å™¨å¯¾å¿œç‰ˆ

import argparse
import sys
import json
import pandas as pd
from pathlib import Path
from config import EVALUATOR_CONFIG, get_enabled_rag_systems, validate_config
from connectors.universal import UniversalRAGConnector
from evaluators.factory import EvaluatorManager

class MultiEvaluatorRAGSystem:
    """å¤šè©•ä¾¡å™¨å¯¾å¿œRAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        # è¨­å®šã‚’æ¤œè¨¼
        config_errors = validate_config()
        if config_errors:
            raise ValueError(f"è¨­å®šã‚¨ãƒ©ãƒ¼: {config_errors}")
        
        # RAGé€£æ¥å™¨ã‚’åˆæœŸåŒ–
        self.connectors = {}
        enabled_systems = get_enabled_rag_systems()
        
        for system_name, config in enabled_systems.items():
            try:
                connector = UniversalRAGConnector(system_name, config)
                if connector.test_connection():
                    self.connectors[system_name] = connector
                    print(f"âœ… {system_name} RAGã‚·ã‚¹ãƒ†ãƒ æ¥ç¶šæˆåŠŸ")
                else:
                    print(f"âŒ {system_name} RAGã‚·ã‚¹ãƒ†ãƒ æ¥ç¶šå¤±æ•—")
            except Exception as e:
                print(f"âŒ {system_name} RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        if not self.connectors:
            raise ValueError("è©•ä¾¡å¯èƒ½ãªRAGã‚·ã‚¹ãƒ†ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # è©•ä¾¡å™¨ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆFactory Patternä½¿ç”¨ï¼‰
        self.evaluator_manager = EvaluatorManager(EVALUATOR_CONFIG)

    def load_test_cases(self, file_path: str) -> list:
        """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise ValueError(f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹èª­ã¿è¾¼ã¿å¤±æ•— {file_path}: {e}")

    def query_all_systems(self, questions: list) -> dict:
        """å…¨RAGã‚·ã‚¹ãƒ†ãƒ ã«è³ªå•"""
        import time
        
        results = {system: [] for system in self.connectors.keys()}
        
        for i, question in enumerate(questions):
            print(f"è³ªå• {i+1}/{len(questions)}: {question[:50]}...")
            
            for system_name, connector in self.connectors.items():
                try:
                    result = connector.query(question)
                    answer = result.get("answer", "")
                    if result.get("error"):
                        print(f"  {system_name} ã‚¨ãƒ©ãƒ¼: {result['error']}")
                        answer = ""
                    else:
                        print(f"  {system_name} âœ… æˆåŠŸ")
                    results[system_name].append(answer)
                    
                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’è¿½åŠ 
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"  {system_name} ä¾‹å¤–: {e}")
                    results[system_name].append("")
        
        return results

    def evaluate_with_all_evaluators(self, questions: list, ground_truths: list,
                                   system_answers: dict) -> dict:
        """å…¨è©•ä¾¡å™¨ã§è©•ä¾¡ï¼ˆFactory Patternä½¿ç”¨ï¼‰"""
        all_results = {}

        for system_name, answers in system_answers.items():
            print(f"\nğŸ“Š {system_name}ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ä¸­...")

            # è©•ä¾¡å™¨ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ã—ã¦å…¨è©•ä¾¡å™¨ã§è©•ä¾¡
            system_results = self.evaluator_manager.evaluate_all(
                questions, answers, ground_truths
            )

            # çµæœã‚’å†æ§‹æˆï¼ˆã‚·ã‚¹ãƒ†ãƒ åˆ¥â†’è©•ä¾¡å™¨åˆ¥ã®æ§‹é€ ã«å¤‰æ›ï¼‰
            for evaluator_name, metrics in system_results.items():
                if evaluator_name not in all_results:
                    all_results[evaluator_name] = {}
                all_results[evaluator_name][system_name] = metrics

        return all_results

    def save_results(self, questions: list, ground_truths: list, system_answers: dict, 
                    evaluation_results: dict, output_dir: str) -> str:
        """çµæœã‚’ä¿å­˜"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # çµæœDataFrameã‚’æ§‹ç¯‰
        results_data = {
            "question": questions,
            "ground_truth": ground_truths
        }
        
        # RAGã‚·ã‚¹ãƒ†ãƒ ã®å›ç­”ã‚’è¿½åŠ 
        for system_name in self.connectors.keys():
            results_data[f"{system_name}_answer"] = system_answers[system_name]
        
        # å„è©•ä¾¡å™¨ã®çµæœã‚’è¿½åŠ 
        for evaluator_name, eval_results in evaluation_results.items():
            for system_name in self.connectors.keys():
                if system_name in eval_results:
                    metrics = eval_results[system_name]
                    for metric_name, scores in metrics.items():
                        col_name = f"{system_name}_{evaluator_name}_{metric_name}"
                        results_data[col_name] = scores
        
        df = pd.DataFrame(results_data)
        csv_path = output_path / "multi_evaluation_results.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        return str(csv_path)

    def run_evaluation(self, test_cases_file: str, output_dir: str = "results") -> str:
        """å®Œå…¨ãªè©•ä¾¡ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ å¤šè©•ä¾¡å™¨RAGè©•ä¾¡é–‹å§‹...")
        
        # 1. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
        test_cases = self.load_test_cases(test_cases_file)
        questions = [case["question"] for case in test_cases]
        ground_truths = [case["ground_truth"] for case in test_cases]
        
        print(f"ğŸ“‹ {len(test_cases)}å€‹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿")
        print(f"ğŸ”Œ RAGã‚·ã‚¹ãƒ†ãƒ : {list(self.connectors.keys())}")
        print(f"ğŸ“Š è©•ä¾¡å™¨: {list(self.evaluator_manager.evaluators.keys())}")
        
        # 2. å…¨RAGã‚·ã‚¹ãƒ†ãƒ ã«è³ªå•
        system_answers = self.query_all_systems(questions)
        
        # 3. å…¨è©•ä¾¡å™¨ã§è©•ä¾¡
        evaluation_results = self.evaluate_with_all_evaluators(questions, ground_truths, system_answers)
        
        # 4. çµæœã‚’ä¿å­˜
        result_file = self.save_results(questions, ground_truths, system_answers, 
                                      evaluation_results, output_dir)
        
        print(f"\nâœ… çµæœã‚’ä¿å­˜: {result_file}")
        return result_file

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="å¤šè©•ä¾¡å™¨RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ„ãƒ¼ãƒ«")
    parser.add_argument(
        "--test-cases", 
        default="data/test_cases_jp.json",
        help="ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/test_cases_jp.json)"
    )
    parser.add_argument(
        "--output", 
        default="results",
        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: results)"
    )
    
    args = parser.parse_args()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    if not Path(args.test_cases).exists():
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.test_cases}")
        sys.exit(1)
    
    try:
        # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        eval_system = MultiEvaluatorRAGSystem()
        
        # è©•ä¾¡ã‚’å®Ÿè¡Œ
        result_file = eval_system.run_evaluation(args.test_cases, args.output)
        
        print(f"\nğŸ‰ å¤šè©•ä¾¡å™¨è©•ä¾¡å®Œäº†ï¼")
        print(f"ğŸ“Š çµæœç¢ºèª: python view_results.py {result_file}")
        
    except Exception as e:
        print(f"âŒ è©•ä¾¡å¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
